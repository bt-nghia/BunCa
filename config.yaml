Youshu:
  data_path: './datasets'
  batch_size_train: 2048 # the batch size for training
  batch_size_test: 2048 # the batch size for testing
  topk: [10, 20, 40, 80] # the topks metrics for evaluation
  neg_num: 1 # number of negatives used for BPR loss. All the experiments use 1.
  aug_type: "ED" # options: ED, MD, OP
  ed_interval: 1 # by how many epochs to dropout edge, default is 1
  embedding_sizes: [64] # the embedding size for user, bundle, and item
  num_layerss: [1] # number of layers for the infomation progagation over the item- and bundle-level graphs
  # the following dropout rates are with respect to the "aug_type", i.e., if aug_type is ED, the following dropout rates are for ED.
  item_level_ratios: [0.2] # the dropout ratio for item-view graph
  bundle_level_ratios: [0.2] # the dropout ratio for bundle-view graph
  bundle_agg_ratios: [0.2] # the dropout ratio for bundle-item affiliation graph
  lrs: [1.0e-3] # learning rate
  l2_regs: [1.0e-4] # the l2 regularization weight: lambda_2
  c_lambdas: [0.04] # the contrastive loss weight: lambda_1
  c_temps: [0.25] # the temperature in the contrastive loss: tau
  self_loop: False
  epochs: 120
  UI_layers: [0.7, 0.3]
  seed: 2023
  UB_coefs: [1, 1]
  UI_coefs: [1, 1]
  BI_coefs: [1, 1]
  sep: "\t"
  file_type: ".txt"
  topk_valid: 20
  nhead: 2
  extra_layer: True

NetEase:
  data_path: './datasets'
  batch_size_train: 2048
  batch_size_test: 2048
  topk: [10, 20, 40, 80]
  neg_num: 1
  aug_type: "ED"
  ed_interval: 1
  embedding_sizes: [64]
  num_layerss: [1]
  lrs: [1.0e-3]
  item_level_ratios: [0.1]
  bundle_level_ratios: [0.3]
  bundle_agg_ratios: [0.1]
  l2_regs: [1.0e-4]
  self_loop: False
  c_lambdas: [0.1]
  c_temps: [0.25]
  epochs: 60
  topk_valid: 20
  UI_layers: [0.7, 0.3]
  seed: 2023
  UB_coefs: [1, 1]
  UI_coefs: [1, 1]
  BI_coefs: [1, 1]
  sep: "\t"
  file_type: ".txt"
  topk_valid: 20
  nhead: 2
  extra_layer: True #Fasle


iFashion:
  data_path: './datasets'
  batch_size_train: 2048
  batch_size_test: 2048
  topk: [10, 20, 40, 80]
  neg_num: 1
  aug_type: "ED"
  ed_interval: 1
  embedding_sizes: [64]
  num_layerss: [1]
  lrs: [1.0e-3]
  item_level_ratios: [0.2]
  bundle_level_ratios: [0.2]
  bundle_agg_ratios: [0]
  l2_regs: [4.0e-5]
  self_loop: False
  c_lambdas: [0.25]
  c_temps: [0.2]
  epochs: 60
  topk_valid: 20
  UI_layers: [0.7, 0.3]
  seed: 2023
  UB_coefs: [1, 1]
  UI_coefs: [1, 1]
  BI_coefs: [1, 1]
  sep: "\t"
  file_type: ".txt"
  topk_valid: 20
  nhead: 2
  extra_layer: True #False


# ------------------- amazon datasets ---------------------
food:
  data_path: './datasets'
  batch_size_train: 128 
  batch_size_test: 128 
  topk: [1, 2, 3, 4, 5]
  neg_num: 1 
  aug_type: "ED" 
  ed_interval: 1 
  embedding_sizes: [64] 
  num_layerss: [1] 
  item_level_ratios: [0.2]
  bundle_level_ratios: [0.2] 
  bundle_agg_ratios: [0.2] 
  self_loop: False
  lrs: [1.0e-3] 
  l2_regs: [1.0e-4] 
  c_lambdas: [0.01] 
  c_temps: [0.25] 
  epochs: 30
  UI_layers: [0.7, 0.3]
  seed: 2023
  UB_coefs: [1, 1]
  UI_coefs: [1, 1]
  BI_coefs: [1, 1]
  sep: "\t"
  file_type: ".txt"
  topk_valid: 3
  nhead: 2
  extra_layer: True

# param 1
# 2023-11-22 14:48:11, Best in epoch 7, TOP 1: REC_V=0.42202, NDCG_V=0.42202
# 2023-11-22 14:48:11, Best in epoch 7, TOP 1: REC_T=0.42661, NDCG_T=0.42661
# 2023-11-22 14:48:11, Best in epoch 7, TOP 3: REC_V=0.85321, NDCG_V=0.68446
# 2023-11-22 14:48:11, Best in epoch 7, TOP 3: REC_T=0.88991, NDCG_T=0.70751
# 2023-11-22 14:48:11, Best in epoch 7, TOP 5: REC_V=0.92661, NDCG_V=0.71486
# 2023-11-22 14:48:11, Best in epoch 7, TOP 5: REC_T=0.92661, NDCG_T=0.72251
# 2023-11-22 14:48:11, Best in epoch 7, TOP 10: REC_V=0.94495, NDCG_V=0.72121
# 2023-11-22 14:48:11, Best in epoch 7, TOP 10: REC_T=0.95872, NDCG_T=0.73365
# 2023-11-22 14:48:11, Best in epoch 7, TOP 20: REC_V=0.97248, NDCG_V=0.72809
# 2023-11-22 14:48:11, Best in epoch 7, TOP 20: REC_T=0.99541, NDCG_T=0.74292

# param 2
# epoch: 11, loss: 0.1461, bpr_loss: 0.1275, c_loss: 1.8591: 100% 10/10 [00:01<00:00,  9.27it/s]
# 2023-11-22 14:55:35, Top_1, Val:  recall: 0.422018, ndcg: 0.422018
# 2023-11-22 14:55:35, Top_1, Test: recall: 0.463303, ndcg: 0.463303
# 2023-11-22 14:55:35, Top_3, Val:  recall: 0.857798, ndcg: 0.688556
# 2023-11-22 14:55:35, Top_3, Test: recall: 0.880734, ndcg: 0.719465
# 2023-11-22 14:55:35, Top_5, Val:  recall: 0.926606, ndcg: 0.717185
# 2023-11-22 14:55:35, Top_5, Test: recall: 0.940367, ndcg: 0.743942
# 2023-11-22 14:55:35, Top_10, Val:  recall: 0.954128, ndcg: 0.726081
# 2023-11-22 14:55:35, Top_10, Test: recall: 0.967890, ndcg: 0.753536
# 2023-11-22 14:55:35, Top_20, Val:  recall: 0.977064, ndcg: 0.731694
# 2023-11-22 14:55:35, Top_20, Test: recall: 0.995413, ndcg: 0.760697


# epoch: 12, loss: 0.1310, bpr_loss: 0.1310, c_loss: 1.9454: 100% 10/10 [00:01<00:00,  9.21it/s]
# 2023-11-22 15:01:23, Top_1, Val:  recall: 0.412844, ndcg: 0.412844
# 2023-11-22 15:01:23, Top_1, Test: recall: 0.458716, ndcg: 0.458716
# 2023-11-22 15:01:23, Top_3, Val:  recall: 0.871560, ndcg: 0.690850
# 2023-11-22 15:01:23, Top_3, Test: recall: 0.885321, ndcg: 0.718865
# 2023-11-22 15:01:23, Top_5, Val:  recall: 0.922018, ndcg: 0.712179
# 2023-11-22 15:01:23, Top_5, Test: recall: 0.940367, ndcg: 0.741567
# 2023-11-22 15:01:23, Top_10, Val:  recall: 0.958716, ndcg: 0.724195
# 2023-11-22 15:01:23, Top_10, Test: recall: 0.967890, ndcg: 0.751056
# 2023-11-22 15:01:23, Top_20, Val:  recall: 0.977064, ndcg: 0.728667
# 2023-11-22 15:01:23, Top_20, Test: recall: 0.995413, ndcg: 0.758325
electronic:
  data_path: './datasets'
  batch_size_train: 128 
  batch_size_test: 128 
  topk: [1, 2, 3, 4, 5]
  neg_num: 1 
  aug_type: "ED" 
  ed_interval: 1 
  embedding_sizes: [64] 
  num_layerss: [1] 
  item_level_ratios: [0.2]
  bundle_level_ratios: [0.2] 
  bundle_agg_ratios: [0.2] 
  self_loop: False
  lrs: [1.0e-3] 
  l2_regs: [1.0e-4] 
  c_lambdas: [0.01] 
  c_temps: [0.25] 
  epochs: 30
  UI_layers: [0.7, 0.3]
  seed: 2023
  UB_coefs: [1, 1]
  UI_coefs: [1, 1]
  BI_coefs: [1, 1]
  sep: "\t"
  topk_valid: 3
  file_type: ".txt"
  nhead: 1
  extra_layer: True

# epoch: 13, loss: 0.1478, bpr_loss: 0.1293, c_loss: 1.8485: 100% 10/10 [00:01<00:00,  9.54it/s]
# 2023-11-22 15:08:12, Top_1, Val:  recall: 0.331776, ndcg: 0.331776
# 2023-11-22 15:08:12, Top_1, Test: recall: 0.443925, ndcg: 0.443925
# 2023-11-22 15:08:12, Top_3, Val:  recall: 0.808411, ndcg: 0.615980
# 2023-11-22 15:08:12, Top_3, Test: recall: 0.845794, ndcg: 0.684628
# 2023-11-22 15:08:12, Top_5, Val:  recall: 0.906542, ndcg: 0.656400
# 2023-11-22 15:08:12, Top_5, Test: recall: 0.920561, ndcg: 0.715804
# 2023-11-22 15:08:12, Top_10, Val:  recall: 0.971963, ndcg: 0.678109
# 2023-11-22 15:08:12, Top_10, Test: recall: 0.971963, ndcg: 0.732556
# 2023-11-22 15:08:12, Top_20, Val:  recall: 0.985981, ndcg: 0.681816
# 2023-11-22 15:08:12, Top_20, Test: recall: 0.981308, ndcg: 0.734959



clothing:
  data_path: './datasets'
  batch_size_train: 128 
  batch_size_test: 128 
  topk: [1, 2, 3, 4, 5]
  neg_num: 1 
  aug_type: "ED" 
  ed_interval: 1 
  embedding_sizes: [64] 
  num_layerss: [1] 
  item_level_ratios: [0.2]
  bundle_level_ratios: [0.2] 
  bundle_agg_ratios: [0.2] 
  self_loop: False
  lrs: [1.0e-3] 
  l2_regs: [1.0e-4] 
  c_lambdas: [0.01] 
  c_temps: [0.25] 
  epochs: 30
  UI_layers: [0.7, 0.3]
  seed: 2023
  UB_coefs: [1, 1]
  UI_coefs: [1, 1]
  BI_coefs: [1, 1]
  sep: "\t"
  file_type: ".txt"
  topk_valid: 3
  nhead: 2
  extra_layer: True

# 2023-11-22 14:38:28, Top_1, Val:  recall: 0.506173, ndcg: 0.506173
# 2023-11-22 14:38:28, Top_1, Test: recall: 0.448560, ndcg: 0.448560
# 2023-11-22 14:38:28, Top_3, Val:  recall: 0.983539, ndcg: 0.804663
# 2023-11-22 14:38:28, Top_3, Test: recall: 0.987654, ndcg: 0.783841
# 2023-11-22 14:38:28, Top_5, Val:  recall: 0.995885, ndcg: 0.809980
# 2023-11-22 14:38:28, Top_5, Test: recall: 0.995885, ndcg: 0.787206
# 2023-11-22 14:38:28, Top_10, Val:  recall: 1.000000, ndcg: 0.811446
# 2023-11-22 14:38:28, Top_10, Test: recall: 1.000000, ndcg: 0.788504
# 2023-11-22 14:38:28, Top_20, Val:  recall: 1.000000, ndcg: 0.811446
# 2023-11-22 14:38:28, Top_20, Test: recall: 1.000000, ndcg: 0.788504